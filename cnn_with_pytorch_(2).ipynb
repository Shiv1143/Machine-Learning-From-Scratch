{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30674,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shiv1143/Machine-Learning-From-Scratch/blob/main/cnn_with_pytorch_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "mK81jZTq8q9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets as dt\n",
        "from torchvision import transforms as T\n",
        "from torch.utils.data import DataLoader as DL\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchvision import models\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "9iKMQ4aQj5A_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accessing GPU"
      ],
      "metadata": {
        "id": "6I418peM82R7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Training on device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9nyaKT3PuN0",
        "outputId": "f2a74d28-6c1c-4b2f-bd5f-5e851ee26c70"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load CIFAR-10 dataset"
      ],
      "metadata": {
        "id": "sZkjYSngOTgZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "R1olIrIHj7Ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = T.Compose(\n",
        "                     [T.ToTensor(),\n",
        "                      T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        "                     )\n",
        "trainset = dt.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = DL(trainset, batch_size=4, shuffle=True)\n",
        "\n",
        "testset = dt.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = DL(testset, batch_size=4, shuffle=False)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-23T18:38:47.551763Z",
          "iopub.execute_input": "2024-03-23T18:38:47.552551Z",
          "iopub.status.idle": "2024-03-23T18:39:04.297198Z",
          "shell.execute_reply.started": "2024-03-23T18:38:47.552520Z",
          "shell.execute_reply": "2024-03-23T18:39:04.296264Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxW9hc8ZOTgc",
        "outputId": "9847deb9-849d-4446-c464-c0f7bdc80951"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 76812777.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t_Z_dJWaktwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computing Accuracy"
      ],
      "metadata": {
        "id": "gBriRM3b86Pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(data_loader, model, device):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    data_iter = iter(data_loader)\n",
        "    with torch.no_grad():\n",
        "      while True:\n",
        "          try:\n",
        "              data = next(data_iter)\n",
        "          except StopIteration:\n",
        "              break\n",
        "          inputs, labels = data[0].to(device), data[1].to(device)\n",
        "          outputs = model(inputs)\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "      accuracy = (correct / total) * 100\n",
        "      print(f'Accuracy on the dataset: {accuracy:.2f}%')\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "mdgExsLzlfai"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-23T18:39:40.281495Z",
          "iopub.execute_input": "2024-03-23T18:39:40.282521Z",
          "iopub.status.idle": "2024-03-23T18:39:40.287835Z",
          "shell.execute_reply.started": "2024-03-23T18:39:40.282484Z",
          "shell.execute_reply": "2024-03-23T18:39:40.287047Z"
        },
        "trusted": true,
        "id": "ltQE5J37OTgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A simple CNN architecture"
      ],
      "metadata": {
        "id": "jVVLIlWtOTgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.convolution1 = nn.Conv2d(3, 32, kernel_size=5, padding=2)\n",
        "        self.convolution2 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
        "        self.pooling = nn.MaxPool2d(2, 2)\n",
        "        self.fully_connected1 = nn.Linear(64 * 8 * 8, 120)\n",
        "        self.fully_connected2 = nn.Linear(120, 60)\n",
        "        self.fully_connected3 = nn.Linear(60, 10)\n",
        "\n",
        "    def forward(self, l):\n",
        "        l = self.pooling(torch.relu(self.convolution1(l)))\n",
        "        l = self.pooling(torch.relu(self.convolution2(l)))\n",
        "        l = l.view(-1, 64 * 8 * 8)\n",
        "        l = torch.relu(self.fully_connected1(l))\n",
        "        l = torch.relu(self.fully_connected2(l))\n",
        "        l = self.fully_connected3(l)\n",
        "        return l\n",
        "\n",
        "\n",
        "\n",
        "# Initializing\n",
        "base_net = CNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(base_net.parameters(), lr=0.0009, momentum=0.8)\n",
        "\n",
        "\n",
        "# Training the basic cnn\n",
        "for e in range(10):\n",
        "    cur_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = base_net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        cur_loss += loss.item()\n",
        "    print(f'Epoch_count: {e + 1}, loss: {cur_loss / len(trainloader)}')\n",
        "\n",
        "print('Training completed')\n",
        "\n",
        "compute_accuracy(testloader, basic_net, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YB-tvNvUpNd6",
        "outputId": "6f619d12-7a2e-4df4-83df-75dc3a41a12b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch_count: 1, loss: 1.6594646263074875\n",
            "Epoch_count: 2, loss: 1.1670529314103724\n",
            "Epoch_count: 3, loss: 0.9399758194772899\n",
            "Epoch_count: 4, loss: 0.7964017690592632\n",
            "Epoch_count: 5, loss: 0.6794104351979681\n",
            "Epoch_count: 6, loss: 0.5733379821524397\n",
            "Epoch_count: 7, loss: 0.48060059355254287\n",
            "Epoch_count: 8, loss: 0.3885987324144511\n",
            "Epoch_count: 9, loss: 0.3103704693638094\n",
            "Epoch_count: 10, loss: 0.24507741102138955\n",
            "Training completed\n",
            "Accuracy on the dataset: 25.86%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25.86"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance Improvement"
      ],
      "metadata": {
        "id": "49FpPW8xOTgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increase the Depth of the Network: Add more convolutional layers to increase the network's capacity to learn more complex features.\n",
        "\n",
        "Batch Normalization: Apply batch normalization after each convolutional layer to stabilize learning and improve convergence speed.\n",
        "\n",
        "Adjust Hyperparameters:\n",
        "Used Adam optimizer instead of SGD.\n",
        "Increase the learning rate initially and then use a learning rate scheduler to decrease it gradually for better convergence.\n",
        "Increase the batch size if the hardware allows, to make the gradient descent smoother.\n",
        "\n",
        "\n",
        "Regularization:\n",
        "Add dropout layers to prevent overfitting.\n",
        "Consider using L2 regularization in the optimizer for further control over model complexity.\n",
        "Use a More Complex Fully Connected Layer: Increase the number of nodes in the hidden layers of the fully connected part to allow the model to learn more complex relationships."
      ],
      "metadata": {
        "id": "R-ZgrLywOTgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImprovedCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImprovedCNN, self).__init__()\n",
        "        # Feature extractor layers\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Classifier layers\n",
        "        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, l):\n",
        "        # Feature extraction\n",
        "        l = F.relu(self.bn1(self.conv1(l)))\n",
        "        l = F.relu(self.bn2(self.conv2(l)))\n",
        "        l = self.pool1(l)\n",
        "        l = F.relu(self.bn3(self.conv3(l)))\n",
        "        l = F.relu(self.bn4(self.conv4(l)))\n",
        "        l = self.pool2(l)\n",
        "\n",
        "        # Flatten the feature maps\n",
        "        l = l.view(-1, 128 * 8 * 8)\n",
        "\n",
        "        # Classification\n",
        "        l = F.dropout(l, p=0.3, training=self.training)\n",
        "        l = F.relu(self.fc1(l))\n",
        "        l = F.dropout(l, p=0.3, training=self.training)\n",
        "        l = self.fc2(l)\n",
        "        return l\n",
        "\n",
        "# Initializing the improved network and optimizer\n",
        "improved_net = ImprovedCNN()\n",
        "\n",
        "improved_net.to(device)\n",
        "\n",
        "optimizer = optim.Adam(improved_net.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler = StepLR(optimizer, step_size=20, gamma=0.7)\n",
        "\n",
        "# Model Training\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = improved_net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, loss: {running_loss / len(trainloader)}')\n",
        "\n",
        "compute_accuracy(testloader, improved_net, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rxVHVtyvd0Y",
        "outputId": "44456fa3-d263-4f01-d342-5916dbb0a29e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, loss: 1.638599201310873\n",
            "Epoch 2, loss: 1.1891960345435142\n",
            "Epoch 3, loss: 0.9882375029551238\n",
            "Epoch 4, loss: 0.8614704246436059\n",
            "Epoch 5, loss: 0.7721889505889593\n",
            "Epoch 6, loss: 0.7042082504128875\n",
            "Epoch 7, loss: 0.6516130230579062\n",
            "Epoch 8, loss: 0.5964381159749854\n",
            "Epoch 9, loss: 0.5529211454967433\n",
            "Epoch 10, loss: 0.5139980134486917\n",
            "Accuracy on the dataset: 75.68%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "75.68"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The effectiveness of the model was significantly enhanced by adding extra layers, incorporating batch normalization, and using dropout for regularization. Additionally, changing the optimizer to Adam, increasing the batch size, and reducing the learning rate from 0.001 to 0.0001 contributed to this improvement. It should be noted that the implementation of a learning rate scheduler also improved the training process. Ultimately, these adjustments resulted in a notable improvement in the model's accuracy on test data, increasing from 25.86% to 75.68% over the same 10 epochs."
      ],
      "metadata": {
        "id": "M5n8geQiOTgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying Transfer Learning"
      ],
      "metadata": {
        "id": "GPrYm5eXOTgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a new classifier\n",
        "class TransferLearningCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TransferLearningCNN, self).__init__()\n",
        "        # Load a pre-trained ResNet model\n",
        "        self.resnet = models.resnet18(pretrained=True)\n",
        "        # Freeze all the parameters in the pre-trained model\n",
        "        for param in self.resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "        # Replace the last fully connected layer with a new one suitable for CIFAR-10\n",
        "        num_ftrs = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "# Initialize the transfer learning model and move it to the device\n",
        "transfer_learning_net = TransferLearningCNN().to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(transfer_learning_net.parameters(), lr=0.001)\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)  # Learning rate scheduler\n",
        "\n",
        "# Training loop\n",
        "for e in range(10):\n",
        "    cur_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = transfer_learning_net(inputs)\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        # Optimize\n",
        "        optimizer.step()\n",
        "\n",
        "        cur_loss += loss.item()\n",
        "    print(f'Epoch {e + 1}, loss: {cur_loss / len(trainloader)}')\n",
        "    scheduler.step()\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Evaluate the model\n",
        "compute_accuracy(testloader, transfer_learning_net, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOLR1avbOTgh",
        "outputId": "5366d082-fa9c-42e9-9453-d86d8a8b768f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 135MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, loss: 2.0779506033468245\n",
            "Epoch 2, loss: 2.0420168658685682\n",
            "Epoch 3, loss: 2.038652170205116\n",
            "Epoch 4, loss: 2.0367823679971693\n",
            "Epoch 5, loss: 2.038052363886833\n",
            "Epoch 6, loss: 1.9675544601106643\n",
            "Epoch 7, loss: 1.9618855648040772\n",
            "Epoch 8, loss: 1.9470566801214217\n",
            "Epoch 9, loss: 1.9423738677358626\n",
            "Epoch 10, loss: 1.9393128563356399\n",
            "Finished Training\n",
            "Accuracy on the test images: 30.49 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MqY_Fc2M0bbe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}